{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1420931,"sourceType":"datasetVersion","datasetId":831855}],"dockerImageVersionId":30715,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ### 'Loan Eligibility Prediction'\n\nIn real life , the problme of loan eligibility prediction is crucial for both lenders and borrowers. For lenders, accurately assesssing the rish associated with each loan application is essential for making informed decision and minimizing financial losses due to defuats. On the other hand for borrowers, access credit can significantly impact their ability to achieve financial goal sunch as buying a home, starting a business or pursiing higher education.\n\n\nThe important of this problme lies in its direct impact on induvidual's financial well-being and the stability of financial institutions. Inaccurate or biased loan eligibility predictions can lead to unfair lending pratices, discrimination and economic.\n\n\nTherefore, Developing reliable and fair loan eligible prediction models is essential for promoting financial inclusion, reducing credit risk and fostering a healthy economy.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:34:00.523689Z","iopub.execute_input":"2024-06-02T10:34:00.524052Z","iopub.status.idle":"2024-06-02T10:34:01.567832Z","shell.execute_reply.started":"2024-06-02T10:34:00.524023Z","shell.execute_reply":"2024-06-02T10:34:01.566688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/loan-eligible-dataset/loan-train.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:34:33.47439Z","iopub.execute_input":"2024-06-02T10:34:33.474781Z","iopub.status.idle":"2024-06-02T10:34:33.500359Z","shell.execute_reply.started":"2024-06-02T10:34:33.474751Z","shell.execute_reply":"2024-06-02T10:34:33.499337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:34:37.819008Z","iopub.execute_input":"2024-06-02T10:34:37.819542Z","iopub.status.idle":"2024-06-02T10:34:37.85144Z","shell.execute_reply.started":"2024-06-02T10:34:37.819499Z","shell.execute_reply":"2024-06-02T10:34:37.850501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Loan_ID: This column contains unique identifiers for each loan application. It is used to uniquely identify each record in the dataset.\n\n2. Gender: This column represents the gender of the applicant, where 'Male' indicates male applicants and 'Female' indicates female applicants.\n\n3. Married: This column indicates whether the applicant is married or not. 'Yes' means the applicant is married, and 'No' means the applicant is not married.\n\n4. Dependents: This column indicates the number of dependents (e.g., children, elderly parents) the applicant has. It typically includes categories such as '0', '1', '2', '3+', representing the number of dependents.\n\n5. Education: This column indicates the educational qualification of the applicant, where 'Graduate' indicates the applicant is a graduate and 'Not Graduate' indicates the applicant is not a graduate.\n\n6. Self_Employed: This column indicates whether the applicant is self-employed or not. 'Yes' means the applicant is self-employed, and 'No' means the applicant is not self-employed.\n\n7. ApplicantIncome: This column represents the income of the applicant.\n\n8. CoapplicantIncome: This column represents the income of the co-applicant (if any) who is applying for the loan with the primary applicant.\n\n9. LoanAmount: This column represents the amount of the loan applied for by the applicant.\n\n10. Loan_Amount_Term: This column represents the term (duration) of the loan in months.\n\n11. Credit_History: This column indicates the credit history of the applicant, where '1' means the applicant has a credit history, and '0' means the applicant does not have a credit history.\n\n12. Property_Area: This column represents the location of the property for which the loan is being applied. It typically includes categories such as 'Rural', 'Semiurban', and 'Urban'.\n\n13. Loan_Status: This column indicates whether the loan application was approved or not. 'Y' means the loan was approved, and 'N' means the loan was not approved.","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:35:25.453468Z","iopub.execute_input":"2024-06-02T10:35:25.45386Z","iopub.status.idle":"2024-06-02T10:35:25.478934Z","shell.execute_reply.started":"2024-06-02T10:35:25.453832Z","shell.execute_reply":"2024-06-02T10:35:25.478127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:35:45.698653Z","iopub.execute_input":"2024-06-02T10:35:45.699036Z","iopub.status.idle":"2024-06-02T10:35:45.727039Z","shell.execute_reply.started":"2024-06-02T10:35:45.699006Z","shell.execute_reply":"2024-06-02T10:35:45.725926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Step 01: Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:36:15.723756Z","iopub.execute_input":"2024-06-02T10:36:15.72418Z","iopub.status.idle":"2024-06-02T10:36:15.733984Z","shell.execute_reply.started":"2024-06-02T10:36:15.724127Z","shell.execute_reply":"2024-06-02T10:36:15.732716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. SimpleImputer: This class from sklearn.impute provides a simple strategy for imputing missing values in a dataset. Missing values can be replaced with a constant value (like 0), the mean, median, or most frequent value along each column. It helps handle missing data before feeding it to machine learning algorithms.\n\n2. OneHotEncoder: This class from sklearn.preprocessing is used for one-hot encoding categorical features. Categorical variables are typically encoded as integers before being fed into machine learning algorithms, but this can introduce unintended ordinality. One-hot encoding transforms categorical variables into a binary matrix where each category becomes a separate binary feature.\n\n3. StandardScaler: This class from sklearn.preprocessing is used for standardizing features by removing the mean and scaling to unit variance. Standardization is a common preprocessing step in machine learning workflows, as it helps to center the data around 0 and scale it to have a standard deviation of 1. This ensures that features are on a similar scale, which can be important for some algorithms.\n\n4. ColumnTransformer: This class from sklearn.compose allows for applying different transformations to different columns or subsets of columns in a dataset. It is particularly useful when you have a mix of numerical and categorical features that require different preprocessing steps. ColumnTransformer enables you to create a preprocessing pipeline that handles each type of feature appropriately.\n\n5. Pipeline: This class from sklearn.pipeline is used to sequentially apply a list of transformations to the data. It chains together multiple processing steps, such as imputation, encoding, and scaling, into a single object. Pipeline provides a convenient way to encapsulate the preprocessing steps and the model training step into a single entity, making the workflow more manageable and less error-prone.","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler,LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:37:02.888504Z","iopub.execute_input":"2024-06-02T10:37:02.888868Z","iopub.status.idle":"2024-06-02T10:37:03.284871Z","shell.execute_reply.started":"2024-06-02T10:37:02.888841Z","shell.execute_reply":"2024-06-02T10:37:03.283676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 1: Define preprocessing steps\nnumeric_features = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term']\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),  # Impute missing values with median\n    ('scaler', StandardScaler())  # Scale numerical features\n])\n\ncategorical_features = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History', 'Property_Area']\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with mode\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode categorical features\n])\n\n# Step 2: Combine preprocessing steps using ColumnTransformer\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\n# Step 3: Fit and transform the data\nprocessed_data = preprocessor.fit_transform(df)\n\n# Step 4: Convert processed_data back to a DataFrame\nprocessed_df = pd.DataFrame(processed_data, columns=numeric_features + list(preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out()))\n\n# Step 5: Add the target variable 'Loan_Status' to the DataFrame\nprocessed_df['Loan_Status'] = df['Loan_Status']","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:37:59.483415Z","iopub.execute_input":"2024-06-02T10:37:59.483789Z","iopub.status.idle":"2024-06-02T10:37:59.50964Z","shell.execute_reply.started":"2024-06-02T10:37:59.483761Z","shell.execute_reply":"2024-06-02T10:37:59.508559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:38:47.423295Z","iopub.execute_input":"2024-06-02T10:38:47.42368Z","iopub.status.idle":"2024-06-02T10:38:47.458511Z","shell.execute_reply.started":"2024-06-02T10:38:47.42365Z","shell.execute_reply":"2024-06-02T10:38:47.457219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Step 02L EDA (Exploritory Data Analysis)**","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nfrom plotly.subplots import make_subplots\n\n# Define the list of categorical features\ncategorical_features = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area']\n\n# Create subplots for each categorical feature\nfig = make_subplots(rows=2, cols=3, subplot_titles=categorical_features)\n\n# Plot count plots for each categorical feature\nfor i, feature in enumerate(categorical_features):\n    counts = df[feature].value_counts().reset_index()\n    counts.columns = [feature, 'count']\n    fig.add_trace(px.bar(counts, x=feature, y='count').data[0], row=(i // 3) + 1, col=(i % 3) + 1)\n\n# Update layout\nfig.update_layout(title='Count of Categorical Features', showlegend=False)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:39:46.153424Z","iopub.execute_input":"2024-06-02T10:39:46.153807Z","iopub.status.idle":"2024-06-02T10:39:48.752882Z","shell.execute_reply.started":"2024-06-02T10:39:46.153776Z","shell.execute_reply":"2024-06-02T10:39:48.751854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the list of categorical features\ncategorical_features = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area']\n\n# Plot grouped bar plots for each categorical feature with Loan_Status\nfor feature in categorical_features:\n    counts = df.groupby([feature, 'Loan_Status']).size().reset_index(name='count')\n    fig = px.bar(counts, x=feature, y='count', color='Loan_Status', barmode='group', \n                 labels={'count': 'Count', 'Loan_Status': 'Loan Status'}, \n                 title=f'Count of Loan Status by {feature}')\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:40:07.818376Z","iopub.execute_input":"2024-06-02T10:40:07.818736Z","iopub.status.idle":"2024-06-02T10:40:08.216369Z","shell.execute_reply.started":"2024-06-02T10:40:07.818709Z","shell.execute_reply":"2024-06-02T10:40:08.215211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Step 03: Study more on feature that influence the decision of Loan Eligibility**","metadata":{}},{"cell_type":"code","source":"print(\"Summary Statistics for ApplicantIncome:\")\nprint(df['ApplicantIncome'].describe())","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:40:39.884076Z","iopub.execute_input":"2024-06-02T10:40:39.884477Z","iopub.status.idle":"2024-06-02T10:40:39.894259Z","shell.execute_reply.started":"2024-06-02T10:40:39.884444Z","shell.execute_reply":"2024-06-02T10:40:39.892807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nsns.histplot(df['ApplicantIncome'], kde=True)\nplt.title('Histogram of ApplicantIncome')\nplt.xlabel('ApplicantIncome')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:40:54.423759Z","iopub.execute_input":"2024-06-02T10:40:54.424137Z","iopub.status.idle":"2024-06-02T10:40:54.925862Z","shell.execute_reply.started":"2024-06-02T10:40:54.42411Z","shell.execute_reply":"2024-06-02T10:40:54.924451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nsns.boxplot(y=df['ApplicantIncome'])\nplt.title('Box Plot of ApplicantIncome')\nplt.ylabel('ApplicantIncome')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:41:10.583245Z","iopub.execute_input":"2024-06-02T10:41:10.583656Z","iopub.status.idle":"2024-06-02T10:41:10.825365Z","shell.execute_reply.started":"2024-06-02T10:41:10.583625Z","shell.execute_reply":"2024-06-02T10:41:10.824131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Summary Statistics for LoanAmount:\")\nprint(df['LoanAmount'].describe())","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:42:54.148372Z","iopub.execute_input":"2024-06-02T10:42:54.148772Z","iopub.status.idle":"2024-06-02T10:42:54.158784Z","shell.execute_reply.started":"2024-06-02T10:42:54.148739Z","shell.execute_reply":"2024-06-02T10:42:54.157652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nsns.histplot(df['LoanAmount'], kde=True)\nplt.title('Histogram of LoanAmount')\nplt.xlabel('LoanAmount')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:43:08.779016Z","iopub.execute_input":"2024-06-02T10:43:08.779428Z","iopub.status.idle":"2024-06-02T10:43:09.179576Z","shell.execute_reply.started":"2024-06-02T10:43:08.779397Z","shell.execute_reply":"2024-06-02T10:43:09.178262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nsns.boxplot(y=df['LoanAmount'])\nplt.title('Box Plot of LoanAmount')\nplt.ylabel('LoanAmount')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:43:24.347489Z","iopub.execute_input":"2024-06-02T10:43:24.34787Z","iopub.status.idle":"2024-06-02T10:43:24.574336Z","shell.execute_reply.started":"2024-06-02T10:43:24.347841Z","shell.execute_reply":"2024-06-02T10:43:24.573113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Summary Statistics for Loan_Amount_Term:\")\nprint(df['Loan_Amount_Term'].describe())","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:43:42.244147Z","iopub.execute_input":"2024-06-02T10:43:42.247467Z","iopub.status.idle":"2024-06-02T10:43:42.257073Z","shell.execute_reply.started":"2024-06-02T10:43:42.247423Z","shell.execute_reply":"2024-06-02T10:43:42.256079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nsns.histplot(df['Loan_Amount_Term'], kde=True)\nplt.title('Histogram of Loan_Amount_Term')\nplt.xlabel('Loan_Amount_Term')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:44:01.949346Z","iopub.execute_input":"2024-06-02T10:44:01.949715Z","iopub.status.idle":"2024-06-02T10:44:02.263133Z","shell.execute_reply.started":"2024-06-02T10:44:01.949687Z","shell.execute_reply":"2024-06-02T10:44:02.261953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Step 04: Remove outlier from those features**","metadata":{}},{"cell_type":"code","source":"# Define the numerical features\nnumerical_features = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term']\n\n# Remove outliers above the 99th percentile for each numerical column\nfor feature in numerical_features:\n    percentile_99 = df[feature].quantile(0.99)\n    df = df[df[feature] <= percentile_99]\n\n# Now you can perform EDA on the updated dataframe without outliers","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:45:39.089929Z","iopub.execute_input":"2024-06-02T10:45:39.090378Z","iopub.status.idle":"2024-06-02T10:45:39.10384Z","shell.execute_reply.started":"2024-06-02T10:45:39.090342Z","shell.execute_reply":"2024-06-02T10:45:39.102688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine numerical and categorical features\nall_features = numerical_features + categorical_features\n\n# Summary statistics after removing outliers\nprint(\"Summary Statistics after Removing Outliers:\")\nprint(df[all_features].describe())","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:45:56.613003Z","iopub.execute_input":"2024-06-02T10:45:56.61343Z","iopub.status.idle":"2024-06-02T10:45:56.634322Z","shell.execute_reply.started":"2024-06-02T10:45:56.613397Z","shell.execute_reply":"2024-06-02T10:45:56.632212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Step 05: Building Logistic Regression Model**","metadata":{}},{"cell_type":"code","source":"class LogisticRegression:\n    def __init__(self, lr=0.1, max_iter=10000):\n        self.lr = lr\n        self.max_iter = max_iter\n        self.weight = None\n        self.bias = None\n        self.cost = []\n\n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n    \n    def cost_function(self, X, y):\n        m = len(y)\n        z = np.dot(X, self.weight) + self.bias\n        h = self.sigmoid(z)\n        cost = -1 / m * (np.dot(y, np.log(h)) + np.dot((1 - y), np.log(1 - h)))\n        return cost\n    \n    def fit(self, X, y):\n        m, n = X.shape\n        self.weight = np.zeros(n)\n        self.bias = 0\n\n        for i in range(self.max_iter + 1):\n            z = np.dot(X, self.weight) + self.bias\n            h = self.sigmoid(z)\n            grad_w = 1 / m * np.dot(X.T, y - h)\n            grad_b = 1 / m * np.sum(y - h)\n\n            self.weight += self.lr * grad_w\n            self.bias += self.lr * grad_b\n\n            c = self.cost_function(X, y)\n            self.cost.append(c)\n\n            final_weight = self.weight.tolist()\n            final_bias = self.weight.tolist()\n\n            if i % 1000 == 0:\n                print(f\"Iteration {i}: Cost = {c}\")\n\n    def predict(self, X):\n        z = np.dot(X, self.weight) + self.bias\n        h = self.sigmoid(z)\n        predictions = [1 if p >= 0.5 else 0 for p in h]\n        return predictions","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:52:03.443659Z","iopub.execute_input":"2024-06-02T10:52:03.444815Z","iopub.status.idle":"2024-06-02T10:52:03.458615Z","shell.execute_reply.started":"2024-06-02T10:52:03.444771Z","shell.execute_reply":"2024-06-02T10:52:03.457108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:52:03.843875Z","iopub.execute_input":"2024-06-02T10:52:03.844558Z","iopub.status.idle":"2024-06-02T10:52:03.849407Z","shell.execute_reply.started":"2024-06-02T10:52:03.844523Z","shell.execute_reply":"2024-06-02T10:52:03.848057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the features (X) and target variable (y)\nX = processed_df.drop(columns=['Loan_Status'])  # Features\ny = processed_df['Loan_Status']  # Target variable\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:52:04.313771Z","iopub.execute_input":"2024-06-02T10:52:04.31418Z","iopub.status.idle":"2024-06-02T10:52:04.322004Z","shell.execute_reply.started":"2024-06-02T10:52:04.314136Z","shell.execute_reply":"2024-06-02T10:52:04.320859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Encode target variable y_train\ny_train_encoded = label_encoder.fit_transform(y_train)\n\n# Train the logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train_encoded)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:52:04.819724Z","iopub.execute_input":"2024-06-02T10:52:04.820119Z","iopub.status.idle":"2024-06-02T10:52:11.452061Z","shell.execute_reply.started":"2024-06-02T10:52:04.82009Z","shell.execute_reply":"2024-06-02T10:52:11.450885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(len(model.cost)), model.cost)\nplt.xlabel('Iteration')\nplt.ylabel('Cost')\nplt.title('Cost vs. Iteration')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:52:33.464749Z","iopub.execute_input":"2024-06-02T10:52:33.465249Z","iopub.status.idle":"2024-06-02T10:52:33.711059Z","shell.execute_reply.started":"2024-06-02T10:52:33.465207Z","shell.execute_reply":"2024-06-02T10:52:33.710203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_numeric = model.predict(X_test)\n\nlabel_mapping = {0: 'N', 1: 'Y'}\ny_pred = np.array([label_mapping[label] for label in y_pred_numeric])\n\n# Step 6: Evaluate the model's performance\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:53:12.408967Z","iopub.execute_input":"2024-06-02T10:53:12.409376Z","iopub.status.idle":"2024-06-02T10:53:12.418506Z","shell.execute_reply.started":"2024-06-02T10:53:12.409344Z","shell.execute_reply":"2024-06-02T10:53:12.417246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Use sklearn logistic model to compare the result**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:53:30.619203Z","iopub.execute_input":"2024-06-02T10:53:30.619602Z","iopub.status.idle":"2024-06-02T10:53:30.624844Z","shell.execute_reply.started":"2024-06-02T10:53:30.619573Z","shell.execute_reply":"2024-06-02T10:53:30.623716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LogisticRegression()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:54:19.343194Z","iopub.execute_input":"2024-06-02T10:54:19.34358Z","iopub.status.idle":"2024-06-02T10:54:19.376077Z","shell.execute_reply.started":"2024-06-02T10:54:19.343548Z","shell.execute_reply":"2024-06-02T10:54:19.373136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n# Calculate confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Plot confusion matrix using seaborn\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['Predicted No', 'Predicted Yes'],\n            yticklabels=['Actual No', 'Actual Yes'])\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:55:14.228295Z","iopub.execute_input":"2024-06-02T10:55:14.228698Z","iopub.status.idle":"2024-06-02T10:55:14.518036Z","shell.execute_reply.started":"2024-06-02T10:55:14.228662Z","shell.execute_reply":"2024-06-02T10:55:14.516855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nreport = classification_report(y_test, y_pred)\n\nprint(\"Report Summarize\")\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:55:34.758857Z","iopub.execute_input":"2024-06-02T10:55:34.75926Z","iopub.status.idle":"2024-06-02T10:55:34.780954Z","shell.execute_reply.started":"2024-06-02T10:55:34.759229Z","shell.execute_reply":"2024-06-02T10:55:34.779379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}